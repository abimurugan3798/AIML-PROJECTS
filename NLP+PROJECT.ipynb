{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bfab555a",
   "metadata": {},
   "source": [
    "# PART A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "c0abde66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, TimeDistributed, Bidirectional, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.initializers import Constant \n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1223e9",
   "metadata": {},
   "source": [
    "### Import and analyse the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "542b279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "44c8e752",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words = vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008e9a16",
   "metadata": {},
   "source": [
    "### Perform relevant sequence adding on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "51acd7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 300\n",
    "train_data = pad_sequences(train_data, maxlen = max_len)\n",
    "test_data = pad_sequences(test_data, maxlen = max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e3f594",
   "metadata": {},
   "source": [
    "### Print shape of features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "c90d071c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features (train_data): (25000, 300)\n",
      "Shape of labels (train_labels): (25000,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of features (train_data):\", train_data.shape)\n",
    "print(\"Shape of labels (train_labels):\", train_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b822de",
   "metadata": {},
   "source": [
    "### Print value of any one feature and it's label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "d64a3659",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example feature (encoded review): [   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    1   14\n",
      "   22   16   43  530  973 1622 1385   65  458 4468   66 3941    4  173\n",
      "   36  256    5   25  100   43  838  112   50  670    2    9   35  480\n",
      "  284    5  150    4  172  112  167    2  336  385   39    4  172 4536\n",
      " 1111   17  546   38   13  447    4  192   50   16    6  147 2025   19\n",
      "   14   22    4 1920 4613  469    4   22   71   87   12   16   43  530\n",
      "   38   76   15   13 1247    4   22   17  515   17   12   16  626   18\n",
      "    2    5   62  386   12    8  316    8  106    5    4 2223 5244   16\n",
      "  480   66 3785   33    4  130   12   16   38  619    5   25  124   51\n",
      "   36  135   48   25 1415   33    6   22   12  215   28   77   52    5\n",
      "   14  407   16   82    2    8    4  107  117 5952   15  256    4    2\n",
      "    7 3766    5  723   36   71   43  530  476   26  400  317   46    7\n",
      "    4    2 1029   13  104   88    4  381   15  297   98   32 2071   56\n",
      "   26  141    6  194 7486   18    4  226   22   21  134  476   26  480\n",
      "    5  144   30 5535   18   51   36   28  224   92   25  104    4  226\n",
      "   65   16   38 1334   88   12   16  283    5   16 4472  113  103   32\n",
      "   15   16 5345   19  178   32]\n",
      "Label for the example feature: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Example feature (encoded review):\", train_data[0])\n",
    "print(\"Label for the example feature:\", train_labels[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08feea1d",
   "metadata": {},
   "source": [
    "### Decode the feature value to get original sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "25b2931f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.concatenate((X_train, X_test), axis=0)\n",
    "label = np.concatenate((y_train, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6e9cda03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoded Review: # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert # is an amazing actor and now the same being director # father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for # and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also # to the two little boy's that played the # of norman and paul they were just brilliant children are often left out of the # list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "word_index = imdb.get_word_index()\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "decoded_review = ' '.join([reverse_word_index.get(i - 3, '#') for i in train_data[0]])\n",
    "print(\"Decoded Review:\", decoded_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638a8d36",
   "metadata": {},
   "source": [
    "### Design, train, tune and test a sequential model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "41f83dcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "157/157 [==============================] - 111s 549ms/step - loss: 0.6014 - accuracy: 0.6865 - val_loss: 0.4057 - val_accuracy: 0.8320\n",
      "Epoch 2/10\n",
      "157/157 [==============================] - 87s 552ms/step - loss: 0.3097 - accuracy: 0.8742 - val_loss: 0.3109 - val_accuracy: 0.8684\n",
      "Epoch 3/10\n",
      "157/157 [==============================] - 77s 489ms/step - loss: 0.2070 - accuracy: 0.9240 - val_loss: 0.3068 - val_accuracy: 0.8728\n",
      "Epoch 4/10\n",
      "157/157 [==============================] - 78s 496ms/step - loss: 0.1548 - accuracy: 0.9468 - val_loss: 0.3455 - val_accuracy: 0.8782\n",
      "Epoch 5/10\n",
      "157/157 [==============================] - 78s 496ms/step - loss: 0.1198 - accuracy: 0.9604 - val_loss: 0.3590 - val_accuracy: 0.8782\n",
      "Epoch 6/10\n",
      "157/157 [==============================] - 82s 524ms/step - loss: 0.0941 - accuracy: 0.9708 - val_loss: 0.3721 - val_accuracy: 0.8700\n",
      "Epoch 7/10\n",
      "157/157 [==============================] - 78s 496ms/step - loss: 0.0741 - accuracy: 0.9778 - val_loss: 0.4381 - val_accuracy: 0.8744\n",
      "Epoch 8/10\n",
      "157/157 [==============================] - 77s 492ms/step - loss: 0.0609 - accuracy: 0.9821 - val_loss: 0.4435 - val_accuracy: 0.8700\n",
      "Epoch 9/10\n",
      "157/157 [==============================] - 77s 492ms/step - loss: 0.0542 - accuracy: 0.9839 - val_loss: 0.5172 - val_accuracy: 0.8652\n",
      "Epoch 10/10\n",
      "157/157 [==============================] - 77s 489ms/step - loss: 0.0378 - accuracy: 0.9898 - val_loss: 0.5404 - val_accuracy: 0.8646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d487570990>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=128, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920e422d",
   "metadata": {},
   "source": [
    "### Use the designed model to print the prediction on any one sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "29dcf4f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 62ms/step\n",
      "Sample Text: # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # # shown in australia as # this incredibly bad movie is so bad that you become # and have to watch it to the end just to see if it could get any worse and it does the storyline is so predictable it seems written by a high school # class the sets are pathetic but # better than the # and the acting is wooden br br the # # seems to have been stolen from the props # of # # there didn't seem to be a single original idea in the whole movie br br i found this movie to be so bad that i laughed most of the way through br br malcolm mcdowell should hang his head in shame he obviously needed the money\n",
      "Prediction (0: Negative, 1: Positive): 0.00052778795\n"
     ]
    }
   ],
   "source": [
    "sample_idx = 20\n",
    "sample_text = train_data[sample_idx]\n",
    "prediction = model.predict(sample_text.reshape(1, -1))\n",
    "print(\"Sample Text:\", ' '.join([reverse_word_index.get(i - 3, '#') for i in sample_text]))\n",
    "print(\"Prediction (0: Negative, 1: Positive):\", prediction[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81a524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73b0af17",
   "metadata": {},
   "source": [
    "# PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138d7bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "383902bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"Sarcasm_Headlines_Dataset.json\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74b25ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_sarcastic</th>\n",
       "      <th>headline</th>\n",
       "      <th>article_link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>thirtysomething scientists unveil doomsday clo...</td>\n",
       "      <td>https://www.theonion.com/thirtysomething-scien...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>dem rep. totally nails why congress is falling...</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/donna-edw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>eat your veggies: 9 deliciously different recipes</td>\n",
       "      <td>https://www.huffingtonpost.com/entry/eat-your-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>inclement weather prevents liar from getting t...</td>\n",
       "      <td>https://local.theonion.com/inclement-weather-p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>mother comes pretty close to using word 'strea...</td>\n",
       "      <td>https://www.theonion.com/mother-comes-pretty-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_sarcastic                                           headline  \\\n",
       "0             1  thirtysomething scientists unveil doomsday clo...   \n",
       "1             0  dem rep. totally nails why congress is falling...   \n",
       "2             0  eat your veggies: 9 deliciously different recipes   \n",
       "3             1  inclement weather prevents liar from getting t...   \n",
       "4             1  mother comes pretty close to using word 'strea...   \n",
       "\n",
       "                                        article_link  \n",
       "0  https://www.theonion.com/thirtysomething-scien...  \n",
       "1  https://www.huffingtonpost.com/entry/donna-edw...  \n",
       "2  https://www.huffingtonpost.com/entry/eat-your-...  \n",
       "3  https://local.theonion.com/inclement-weather-p...  \n",
       "4  https://www.theonion.com/mother-comes-pretty-c...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac905b81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28619"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2ed6ba1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28619 entries, 0 to 28618\n",
      "Data columns (total 3 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   is_sarcastic     28619 non-null  int64 \n",
      " 1   headline         28619 non-null  object\n",
      " 2   headline_length  28619 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 670.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "3a79a423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['is_sarcastic'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b9b99120",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "is_sarcastic       0\n",
       "headline           0\n",
       "headline_length    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0860aab",
   "metadata": {},
   "source": [
    "### Retain relevant columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac5f0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['is_sarcastic', 'headline']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cc711",
   "metadata": {},
   "source": [
    "### Get length of each sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94e6a468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['headline_length'] = df['headline'].apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dd63cd",
   "metadata": {},
   "source": [
    "### Define parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ec0b3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 10000  \n",
    "max_sequence_length = 30  \n",
    "embedding_dim = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63e6678",
   "metadata": {},
   "source": [
    "### Get indices for words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f49c9275",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['headline'])\n",
    "word_index = tokenizer.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad06b5d",
   "metadata": {},
   "source": [
    "### Create features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b5c9c9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tokenizer.texts_to_sequences(df['headline'])\n",
    "X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "y = df['is_sarcastic']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd311867",
   "metadata": {},
   "source": [
    "### Get vocabulary size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "766e3cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2fdbc9a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30885"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ce7858",
   "metadata": {},
   "source": [
    "### Create a weight matrix using GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5aa71cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "zip_path = r'C:\\Users\\Dell\\OneDrive\\glove.6B.zip'\n",
    "extract_folder = r'C:\\Users\\Dell\\OneDrive\\glove.6B'\n",
    "\n",
    "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_folder)\n",
    "\n",
    "embedding_file_path = r'C:\\Users\\Dell\\OneDrive\\glove.6B\\glove.6B.100d.txt'  # or adjust to the specific file you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "782ba8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_index = {}\n",
    "with open(r'C:\\Users\\Dell\\OneDrive\\glove.6B\\glove.6B.100d.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = coefs\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9ebb76",
   "metadata": {},
   "source": [
    "### Define and compile a Bidirectional LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2d480146",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=max_sequence_length, trainable=False))\n",
    "model.add(Bidirectional(LSTM(64, return_sequences=True)))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df8dffd",
   "metadata": {},
   "source": [
    "### Fit the model and check the validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c8c6d516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "358/358 [==============================] - 55s 119ms/step - loss: 0.4908 - accuracy: 0.7590 - val_loss: 0.4033 - val_accuracy: 0.8134\n",
      "Epoch 2/5\n",
      "358/358 [==============================] - 37s 103ms/step - loss: 0.3679 - accuracy: 0.8350 - val_loss: 0.3500 - val_accuracy: 0.8452\n",
      "Epoch 3/5\n",
      "358/358 [==============================] - 36s 101ms/step - loss: 0.3198 - accuracy: 0.8594 - val_loss: 0.3445 - val_accuracy: 0.8524\n",
      "Epoch 4/5\n",
      "358/358 [==============================] - 36s 100ms/step - loss: 0.2870 - accuracy: 0.8767 - val_loss: 0.3272 - val_accuracy: 0.8548\n",
      "Epoch 5/5\n",
      "358/358 [==============================] - 36s 100ms/step - loss: 0.2529 - accuracy: 0.8946 - val_loss: 0.3250 - val_accuracy: 0.8599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2d4e099f090>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=5, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6fa380",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "250a30b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179/179 [==============================] - 4s 23ms/step - loss: 0.3250 - accuracy: 0.8599\n",
      "Validation Accuracy: 85.99%\n"
     ]
    }
   ],
   "source": [
    "_, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Validation Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4886342",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
